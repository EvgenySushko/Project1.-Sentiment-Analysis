{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models\n",
    "by Evgeny Sushko\n",
    "\n",
    "---\n",
    "## Table of Contents:\n",
    "1. Model evaluation applications\n",
    "   - Generalization performance\n",
    "   - Model selection\n",
    "   - Algorithm selection\n",
    "2. Model evaluation techniques\n",
    "   - Holdout validation\n",
    "   - K-fold cross-validation\n",
    "3. Classification metrics\n",
    "   - Accuracy\n",
    "   - Confusion matrix\n",
    "   - Precision & Recall\n",
    "   - F-1 score\n",
    "   - Classification report\n",
    "4. Appropriate merics choice\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Evaluation Applications\n",
    "Let's start with a question: **\"Why do we care about performance estimates at all?\"**\n",
    "\n",
    "1.1. **Generalization performance** - We want to estimate the predictive performance of our model on future (unseen) data.\n",
    "- Ideally, the estimated performance of a model tells **how well it performs on unseen data** – making predictions on future data is often the main problem we want to solve.\n",
    "\n",
    "1.2. **Model selection** - We want to increase the predictive performance by tweaking the learning algorithm and selecting the best performing model from a given hypothesis space.\n",
    "- Typically, machine learning involves a lot of experimentation. Running a learning algorithm over a training dataset with different hyperparameter settings and different features will result in different models. Since we are typically interested in **selecting the best-performing model** from this set, we need to find a way to estimate their respective performances in order to rank them against each other.\n",
    "\n",
    "1.3. **Algorithm selection** - We want to compare different ML algorithms, selecting the best-performing one.\n",
    "- We are usually not only experimenting with the one single algorithm that we think would be the “best solution” under the given circumstances. More often than not, we want to **compare different algorithms to each other**, oftentimes in terms of predictive and computational performance.\n",
    "\n",
    "Although these three sub-tasks have all in common that we want to estimate the performance of a model, they all require different approaches. \n",
    "\n",
    "This tutorial will focus on **supervised learning**, a subcategory of machine learning where our target values are known in our available dataset. Although many concepts also apply to regression analysis, we will focus on **classification**, the assignment of categorical target labels to the samples.\n",
    "\n",
    "---\n",
    "### 2. Model Evaluation Techniques\n",
    "#### 2.1. Holdout validation\n",
    "The holdout method is the simplest model evaluation technique. We take our labeled dataset and split it randomly into two parts: A **training set** and a **test set**\n",
    "![](https://sebastianraschka.com/images/blog/2016/model-evaluation-selection-part1/testing_01.png)\n",
    "Then, we fit a model to the training data and predict the labels of the test set.\n",
    "![](https://sebastianraschka.com/images/blog/2016/model-evaluation-selection-part1/testing_02.png)\n",
    "And the fraction of correct predictions constitutes our estimate of the prediction accuracy.\n",
    "![](https://sebastianraschka.com/images/blog/2016/model-evaluation-selection-part1/testing_03.png)\n",
    "We really don’t want to train and evaluate our model on the same training dataset, since it would introduce **overfitting**. In other words, we can’t tell whether the model simply memorized the training data or not, or whether it generalizes well to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152610"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('movie_reviews.csv')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = df.text, df.label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit a model to the training data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "pipeline = Pipeline([('vectorizer', vectorizer),\n",
    "                     ('classifier', classifier)])\n",
    "\n",
    "model = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict the labels of the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.820064215975\n"
     ]
    }
   ],
   "source": [
    "# calculate prediction accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "print (\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. K-fold Cross-validation\n",
    "K-fold Cross-validation is probably the most common technique for model evaluation and model selection. \n",
    "- We split the dataset into *K* parts and iterate over a dataset set *K* times\n",
    "- In each round one part is used for validation, and the remaining *K-1* parts are merged into a training subset for model evaluation\n",
    "![](https://sebastianraschka.com/images/blog/2016/model-evaluation-selection-part3/kfold.png)\n",
    "- We compute the cross-validation performance as the arithmetic mean over the *K* performance estimates from the validation sets.\n",
    "- Runs \"K\" times slower than simple train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## тут должен быть пример кросс-валидации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classification metrics overview\n",
    "Classification problems are probably the most common type of ML problem and as such there are many metrics that can be used to evaluate predictions for these problems. We will review some of them.\n",
    "\n",
    "#### 3.1. Accuracy\n",
    "Accuracy simply measures *what percent of your predictions were correct*. It's the ratio between the number of correct predictions and the total number of predictions.\n",
    "\n",
    "$$ \\mbox{accuracy} = \\frac{\\mbox{# correct}}{\\mbox{# predictions}} $$\n",
    "\n",
    "This is the most common evaluation metric for classification problems and the easiest to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.818032894306\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is also the most misused metric. It is really **only suitable** when there are an *equal number of observations in each class* (which is rarely the case) and that all *predictions and prediction errors are equally important*, which is often not the case.\n",
    "\n",
    "#### 3.2. Confusion Matrix\n",
    "The confusion matrix is a handy presentation of the accuracy of a model with 2 or more classes. The table **presents predictions** on the x-axis and **accuracy outcomes** on the y-axis. The cells of the table are the number of predictions made by a machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9324  3266]\n",
      " [ 2288 15644]]\n"
     ]
    }
   ],
   "source": [
    "# first argument is true values, second argument is predicted values\n",
    "# this produces a 2x2 numpy array (matrix)\n",
    "conf = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                | Predicted Negative | Predicted Positive |\n",
    "|:--------------:|--------------------|--------------------|\n",
    "| **Negative Cases** |      TN: 9324      |      FP: 3266      |\n",
    "| **Positive Cases** |      FN: 2288      |      TP: 15644     |\n",
    "\n",
    "- ##### True Positives (TP):\n",
    "We correctly predicted that the reviews are positive: **15644**\n",
    "- ##### True Negatives (TN):\n",
    "We correctly predicted that the reviews are negative: **9324**\n",
    "- ##### False Positives (FP):\n",
    "We incorrectly predicted that the reviews are positive: **3266**\n",
    "- ##### False Negatives (FN):\n",
    "We incorrectly predicted that the reviews are negative: **2288**\n",
    "\n",
    "\n",
    "\n",
    "Confusion matrix allows you to compute various classification metrics, and these metrics can guide your model selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# slice confusion matrix into four pieces for future use\n",
    "TP = conf[1, 1]\n",
    "TN = conf[0, 0]\n",
    "FP = conf[0, 1]\n",
    "FN = conf[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can learn more about the [Confusion Matrix on the Wikipedia article](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "\n",
    "#### 3.3. Precision & Recall\n",
    "Precision and recall are actually two metrics. But they are often used together.\n",
    "\n",
    "**Precision** answers the question: *What percent of positive predictions were correct?*\n",
    "\n",
    "$$\\mbox{precision} = \\frac{\\mbox{# true positive}}{\\mbox{#true positive + #false positive}}$$\n",
    "\n",
    "**Recall** answers the question: *What percent of the positive cases did you catch?*\n",
    "\n",
    "$$\\mbox{recall} = \\frac{\\mbox{# true positive}}{\\mbox{#true positive + #false negative}}$$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827287149656\n",
      "0.827287149656\n"
     ]
    }
   ],
   "source": [
    "# calculate precision\n",
    "precision = TP / float(TP + FP)\n",
    "\n",
    "print(precision)\n",
    "print(metrics.precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.872406870399\n",
      "0.872406870399\n"
     ]
    }
   ],
   "source": [
    "# calculate recall\n",
    "recall = TP / float(FN + TP)\n",
    "\n",
    "print(recall)\n",
    "print(metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also a very good explanation of [Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) in Wikipedia.\n",
    "![](http://www.kdnuggets.com/images/precision-recall-relevant-selected.jpg)\n",
    "\n",
    "#### 3.4 F1-score\n",
    "The F1-score (sometimes known as the balanced F-beta score) is a single metric that combines both precision and recall via their harmonic mean:\n",
    "\n",
    "$$F_1 = 2 \\frac{\\mbox{precision} * \\mbox{recall}}{\\mbox{precision + recall}}$$\n",
    "\n",
    "Unlike the arithmetic mean, the harmonic mean tends toward the smaller of the two elements. Hence the F1 score will be small if either precision or recall is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.849248140709\n",
      "0.849248140709\n"
     ]
    }
   ],
   "source": [
    "# calculate f1-score\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(f1)\n",
    "print(metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5. Classification Report\n",
    "Scikit-learn does provide a convenience report when working on classification problems to give you a quick idea of the accuracy of a model using a number of measures.\n",
    "\n",
    "The **classification_report()** function displays the precision, recall, f1-score and support for each class. (*support* is the number of occurrences of each class in *y_true*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.74      0.77     12590\n",
      "          1       0.83      0.87      0.85     17932\n",
      "\n",
      "avg / total       0.82      0.82      0.82     30522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print a report on the binary classification problem\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Choice of Metrics\n",
    "Depending on your application, you may want to consider different performance metrics. Choice of metric depends on your business objective and on the data you have at hand.\n",
    "\n",
    "In many cases **accuracy** alone will be enough. It is suitable when the data is balanced (equal number of observations in each class) and when minimizing *False Positives* and *False Negatives* is equally important.\n",
    "\n",
    "If that is not the case:\n",
    "\n",
    "- Identify if FP or FN is more important to reduce\n",
    "- Choose metric with relevant variable (FP or FN) in the equation\n",
    "\n",
    "##### Case 1: Spam filter (positive class is \"spam\")\n",
    "FN (spam goes to the inbox) are more acceptable than FP (non-spam is caught by the spam filter) => Choose **FP** as a variable, optimize for **precision**\n",
    "\n",
    "##### Case 2: Fraudulent transaction detector (positive class is \"fraud\")\n",
    "FP (normal transactions that are flagged as possible fraud) are more acceptable than FN (fraudulent transactions that are not detected) => Choose **FN** as a variable, optimize for **recall**\n",
    "\n",
    "---\n",
    "#### References\n",
    "- Sebastian Raschka, [Model evaluation, model selection, and algorithm selection in machine learning](https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html)\n",
    "- Jason Brownlee, [Metrics To Evaluate Machine Learning Algorithms in Python](http://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/)\n",
    "- Ritchie Ng, [Evaluating a Classification Model](http://www.ritchieng.com/machine-learning-evaluate-classification-model/)\n",
    "- [Turi Machine Learning Platform User Guide](https://turi.com/learn/userguide/evaluation/classification.html)\n",
    "- Gregory Piatetsky, [21 Must-Know Data Science Interview Questions and Answers](http://www.kdnuggets.com/2016/02/21-data-science-interview-questions-answers.html/2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
